{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic of Information Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block aims to import all the relevant libraries to analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information content\n",
    "\n",
    "function infocontent(p)\n",
    "Computes the Shannon information content for an outcome x of a random variable\n",
    "X with probability p.\n",
    "\n",
    "Inputs:\n",
    "- p - probability to compute the Shannon info content for\n",
    "\n",
    "Outputs:\n",
    "- result - Shannon info content of the probability p\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infocontent(p):    \n",
    "    \n",
    "    frac_logarithm = lambda x:np.log2(1/x)\n",
    "    vfunc = np.vectorize(frac_logarithm)\n",
    "    content = vfunc(p)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# For scalar p:\n",
    "    # return np.log2(1/p)\n",
    "\n",
    "\n",
    "# Comment for newbies**\n",
    "#def frac_logarithm(x):\n",
    "    \n",
    "    #x = np.array(x)\n",
    "    #log = np.log2(1/x)\n",
    "    #return log\n",
    "    \n",
    "#def infocontent(p):\n",
    "    \n",
    "    #content = frac_logarithm(p)\n",
    "    #return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute using your function:<br>\n",
    "- h(heads) for a fair coin?<br>\n",
    "- h(1) for a 6-sided die? h(not 1) for a 6-sided die?<br>\n",
    "- h(1) for a 20-sided die? h(not 1) for a 20-sided die?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code\n",
    "\n",
    "p_1 = 0.5\n",
    "p_2 = 1/6\n",
    "p_2_2 = 5/6\n",
    "p_3 = 1/20\n",
    "p_3_1 = 19/20\n",
    "\n",
    "print(infocontent(p_1))\n",
    "print(infocontent(p_2))\n",
    "print(infocontent(p_2_2))\n",
    "print(infocontent(p_3))\n",
    "print(infocontent(p_3_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "function entropy(p)\n",
    "\n",
    "Computes the Shannon entropy over all outcomes x of a random variable\n",
    "X with probability vector p(x) for each candidate outcome x.\n",
    "\n",
    "Inputs:\n",
    "- p - probability distribution function over all outcomes x.\n",
    "       p is a vector, e.g. p = [0.25, 0.75], the sum over which must be 1.\n",
    "\n",
    "Outputs:\n",
    "- result - Shannon entropy of the probability distribution p\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):  \n",
    "    if type(p) == np.array:\n",
    "        weightedShannonInfos = p*(infocontent(p))\n",
    "    \n",
    "    else:\n",
    "        p = np.array(p)\n",
    "        weightedShannonInfos = p*(infocontent(p))\n",
    "    \n",
    "    contributions = np.sum(weightedShannonInfos)\n",
    "    \n",
    "    return np.round(contributions,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.811278"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [0.25, 0.75]\n",
    "\n",
    "entropy(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function jointentropy(p)\n",
    "\n",
    "Computes the joint Shannon entropy over all outcome vectors x of a vector\n",
    "random variable X with probability matrix p(x) for each candidate outcome\n",
    "vector x.\n",
    "Inputs:\n",
    "- p - probability distribution function over all outcome vectors x.\n",
    "       p is a matrix over all combinations of the sub-variables of x,\n",
    "\twhere p(1,3) gives the probability of the first symbol of sub-variable\n",
    "\tx1 co-occuring with the third symbol of sub-variable x2.\n",
    "       E.g. p = np.array([[0.2,0.3],[0.1,0.4]]). The sum over p must be 1.\n",
    "\n",
    "Outputs:\n",
    "- result - joint Shannon entropy of the probability distribution p\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jointentropy(p):\n",
    "    \n",
    "    # Do I have to test whether this sums up 1?\n",
    "    \n",
    "    return entropy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_jent = np.array([[0.2, 0.3],[ 0.1, 0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.846439"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jointentropy(p_jent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function mutualinformation(p)\n",
    "\n",
    "Computes the mutual information over all outcomes x of a random\n",
    "variable X with outcomes y of a random variable Y.\n",
    "Probability matrix p(x,y) is given for each candidate outcome\n",
    "(x,y).\n",
    "\n",
    "Inputs:\n",
    "- p - 2D probability distribution function over all outcomes (x,y).\n",
    "    p is a matrix over all combinations of x and y,\n",
    "    where p(1,3) gives the probability of the first symbol of variable\n",
    "    x co-occuring with the third symbol of variable y.\n",
    "     E.g. p = np.array([[0.2, 0.3],[ 0.1, 0.4]]). The sum over p must be 1.\n",
    "\n",
    "Outputs:\n",
    "- result - mutual information of X with Y\n",
    " \n",
    "Copyright (C) 2017, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualinformation(p):\n",
    "    \n",
    "    # We need to compute H(X) + H(Y) - H(X,Y):\n",
    "    # 1. joint entropy:\n",
    "    H_XY = jointentropy(p)\n",
    "\n",
    "    # 2. marginal entropy of X:\n",
    "    # But how to get p_x???\n",
    "    p_x = p.sum(axis=1)  # Since x changes along the rows, summing /\n",
    "                    # over the y's (dimension 2 argument in the sum) will just return p(x)\n",
    "    \n",
    "    H_X = entropy(p_x);\n",
    "\n",
    "    # 2. marginal entropy of Y:\n",
    "    # But how to get p_y???\n",
    "    p_y = p.sum(axis=0); # Since y changes along the columns, summing over the x's (dimension 1 argument in the sum) will just return p(y)\n",
    "\n",
    "    H_Y = entropy(p_y);\n",
    "    \n",
    "    mutualInfo = H_X + H_Y - H_XY\n",
    "    \n",
    "    return np.round(mutualInfo,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function conditionalentropy(p)\n",
    "\n",
    "Computes the conditional Shannon entropy over all outcomes x of a random\n",
    "variable X, given outcomes y of a random variable Y.\n",
    "Probability matrix p(x,y) is given for each candidate outcome (x,y).\n",
    "\n",
    "Inputs:\n",
    "- p - 2D probability distribution function over all outcomes (x,y).\n",
    "      p is a matrix over all combinations of x and y,\n",
    "where p(1,3) gives the probability of the first symbol of variable\n",
    "x co-occuring with the third symbol of variable y.\n",
    "E.g. p = np.array([[0.2, 0.3],[ 0.1, 0.4]]). The sum over p must be 1.\n",
    "\n",
    "Outputs:\n",
    "- result - conditional Shannon entropy of X given Y\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalentropy(p):\n",
    "    \n",
    "    p = np.array(p)\n",
    "\n",
    "    H_XY = jointentropy(p)\n",
    "    \n",
    "    p1, p2 = p.sum(axis=0) \n",
    "    p_primma = np.array([p1,p2])\n",
    "    \n",
    "    H_Y = entropy(p_primma)\n",
    "\n",
    "    return H_XY-H_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function conditionalmutualinformation(p)\n",
    "\n",
    "Computes the mutual information over all outcomes x of a random\n",
    "variable X with outcomes y of a random variable Y, conditioning on \n",
    "outcomes z of a random variable Z.\n",
    "Probability matrix p(x,y,z) is given for each candidate outcome\n",
    "(x,y,z).\n",
    "\n",
    "Inputs:\n",
    "- p - 3D probability distribution function over all outcomes (x,y,z).\n",
    "       p is a matrix over all combinations of x and y and z,\n",
    "\twhere p(1,3,2) gives the probability of the first symbol of variable\n",
    "\tx co-occuring with the third symbol of variable y and the second\n",
    "\tsymbol of z.\n",
    "    The sum over p must be 1.\n",
    "    E.g.:\n",
    "        p[:,:,1] = np.array([0.114286, 0.171429],[ 0.057143, 0.228571])\n",
    "        p[:,:,1] = np.array([0.171429, 0.114286],[ 0.028571, 0.114286])\n",
    "\n",
    "Outputs:\n",
    "- result - mutual information of X with Y\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalmutualinformation(p):\n",
    "    \n",
    "    # 1. Joint Entropy\n",
    "    H_XYZ = jointentropy(p)\n",
    "    \n",
    "    # 2. entropy of X,Z:\n",
    "    # But how to get p_xz???\n",
    "    # Sum p over the y's (dimension 2 argument in the sum) will just return p(x,z) terms. \n",
    "    # Won't be a 2D array, but fine to compute entropy on\n",
    "    \n",
    "    p_xz = p.sum(axis=1)\n",
    "    H_XZ = jointentropy(p_xz)\n",
    "    \n",
    "    #3. entropy of Y,Z:\n",
    "    # But how to get p_yz???\n",
    "    # Sum p over the x's (dimension 1 argument in the sum) will just return p(y,z) terms. \n",
    "    # Won't be a 2D array, but fine to compute entropy on\n",
    "    \n",
    "    p_yz = p.sum(axis=0)\n",
    "    H_YZ = jointentropy(p_yz)\n",
    "    \n",
    "    # 4. marginal entropy of Z:\n",
    "    # But how to get p_z???\n",
    "    # Sum p_xz over the x's (dimension 1 argument in the sum) will just return p(z) terms. \n",
    "    # Won't be a 1D array, but fine to compute entropy on\n",
    "    \n",
    "    p_z = p_xz.sum(axis=0)\n",
    "    H_Z = jointentropy(p_z)\n",
    "    \n",
    "    # 5. Computing Conditional Mutual Information\n",
    "    condMutInf = H_XZ - H_Z + H_YZ - H_XYZ\n",
    "    \n",
    "    return np.round(condMutInf, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotentropyX():\n",
    "    \n",
    "    alpha = np.arange(0,1,0.001)\n",
    "    A = np.array([[1-alpha[0],alpha[0]/2],[0,alpha[0]/2]])\n",
    "    \n",
    "    for elem in alpha:\n",
    "        A = np.append(A,np.array([[1-elem,elem/2],[0,elem/2]]),axis=0) \n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.    , 0.    ],\n",
       "       [0.    , 0.    ],\n",
       "       [1.    , 0.    ],\n",
       "       ...,\n",
       "       [0.    , 0.499 ],\n",
       "       [0.001 , 0.4995],\n",
       "       [0.    , 0.4995]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotentropyX()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function entropyempirical($x_{n}$)\n",
    "\n",
    "Computes the Shannon entropy over all outcomes x of a random variable\n",
    "X from samples x_n.\n",
    "\n",
    "Inputs:\n",
    "- xn - samples of outcomes x.\n",
    "      xn is a column vector, e.g. xn = [0;0;1;0;1;0;1;1;1;0] for a binary variable.\n",
    "\n",
    "Outputs:\n",
    "- result - Shannon entropy over all outcomes\n",
    "\n",
    "Copyright (C) 2020, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyempirical(xn):\n",
    "    \n",
    "    # We need to work out the alphabet here.\n",
    "    # The following returns a vector of the alphabet:\n",
    "    \n",
    "    if type(xn) == list:\n",
    "        xn = np.array(xn)\n",
    "    \n",
    "    if xn.ndim == 1:\n",
    "        xn = np.reshape(xn,(1,len(xn))) #reshaping our 1-dim vector to numpy format of a row vector\n",
    "        xn = xn.T # Transposing the row vector to a column vector\n",
    "    \n",
    "    [c1,r1] = xn.shape \n",
    "    denominator = c1\n",
    "    \n",
    "    if c1 == 1:\n",
    "        xn = xn.T\n",
    "        [r1,c1] = xn.shape\n",
    "        denominator = r1\n",
    "        \n",
    "    unique_rows = np.unique(xn, axis=0)\n",
    "    probabilities = []\n",
    "    for unique_row in unique_rows:\n",
    "        count = 0\n",
    "        for row in xn:\n",
    "            if (row==unique_row).all():\n",
    "                count += 1       \n",
    "        probabilities.append(count/denominator)\n",
    "\n",
    "    empEntropy = entropy(probabilities)\n",
    "    return np.round(empEntropy,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xn = np.array([[0,0,1,0,1,0,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropyempirical(xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function jointentropyempirical(xn, yn)\n",
    "\n",
    "Computes the Shannon entropy over all outcome vectors x of a vector random\n",
    "variable X from sample vectors x_n. User can call with two such arguments \n",
    "if they don't wish to join them outside of the call.\n",
    "\n",
    "Inputs:\n",
    "- xn - matrix of samples of outcomes x. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate X.\n",
    "- yn - as per xn, except that yn is not required to be supplied (in which\n",
    "     case the entropy is only calculated over the xn variable).\n",
    "\n",
    "Outputs:\n",
    "- result - joint Shannon entropy over all samples\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier. Distributed under GNU General Public License v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jointentropyempirical(xn, yn):\n",
    "    \n",
    "    xn = np.array(xn)\n",
    "    yn = np.array(yn)\n",
    "    \n",
    "    if xn.ndim == 1:\n",
    "        xn = np.reshape(xn,(1,len(xn)))\n",
    "    \n",
    "    if yn.ndim == 1:\n",
    "        yn = np.reshape(yn,(1,len(yn)))\n",
    "    \n",
    "    [c1,r1] = xn.shape\n",
    "    [c2,r2] = yn.shape\n",
    "    \n",
    "    if c1 == 1 and c2 == 1:\n",
    "        XY = np.concatenate((xn,yn), axis=0)\n",
    "        jointEntrEmp = entropyempirical(XY.T)\n",
    "        return jointEntrEmp\n",
    "    \n",
    "    elif c1 != c2:\n",
    "        XY = np.row_stack((xn,yn))\n",
    "        jointEntrEmp = entropyempirical(XY.T)\n",
    "        return jointEntrEmp\n",
    "    \n",
    "    else:\n",
    "        XY = np.concatenate((xn,yn),axis=1)\n",
    "        jointEntrEmp = entropyempirical(XY)\n",
    "        return jointEntrEmp\n",
    "\n",
    "\n",
    "def jointentropyempiricalMV(xn):\n",
    "    \n",
    "    jointEntrEmp = entropyempirical(xn)\n",
    "    return jointEntrEmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function mutualinformationempirical($x_{n},y_{n}$)\n",
    "\n",
    "Computes the mutual information over all samples $x_{n}$ of a random\n",
    "variable $X$ with samples $y_{n}$ of a random variable $Y$.\n",
    "\n",
    "Inputs:\n",
    "- xn - matrix of samples of outcomes x. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate X.\n",
    "- yn - matrix of samples of outcomes x. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate Y.\n",
    "       Must have the same number of rows as X.\n",
    "\n",
    "Outputs:\n",
    "- result - mutual information of X with Y\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier. Distributed under GNU General Public License v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualinformationempirical(xn,yn):\n",
    "    # We need to compute H(X) + H(Y) - H(X,Y):\n",
    "    # 1. joint entropy:\n",
    "    H_XY = jointentropyempirical(xn, yn);\n",
    "    # 2. marginal entropy of Y: (calling 'joint' in case yn is multivariate)\n",
    "    H_Y = entropyempirical(yn);\n",
    "    #3. marginal entropy of X: (calling 'joint' in case yn is multivariate)\n",
    "    H_X = entropyempirical(xn);\n",
    "\n",
    "    result = H_X + H_Y - H_XY;\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function conditionalentropyempirical(xn,yn)\n",
    "\n",
    "Computes the conditional Shannon entropy over all samples xn of a random\n",
    "variable X, given samples yn of a random variable Y.\n",
    "\n",
    "Inputs:\n",
    "- xn - matrix of samples of outcomes x. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate X.\n",
    "- yn - matrix of samples of outcomes x. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate Y.\n",
    "       Must have the same number of rows as X.\n",
    "\n",
    "Outputs:\n",
    "- result - conditional Shannon entropy of X given Y\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier. Distributed under GNU General Public License v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalentropyempirical(xn,yn):\n",
    "    \n",
    "    # We need to compute H(X,Y) - H(X):\n",
    "    # 1. joint entropy:\n",
    "    H_XY = jointentropyempirical(xn, yn);\n",
    "    # 2. marginal entropy of Y: (calling 'joint' in case yn is multivariate)\n",
    "    H_Y = jointentropyempiricalMV(yn);\n",
    "\n",
    "    result = H_XY - H_Y;\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function conditionalmutualinformationempirical(xn,yn)\n",
    "\n",
    "Computes the mutual information over all samples xn of a random\n",
    "variable X with samples yn of a random variable Y, conditioning on \n",
    "samples zn of a random variable Z.\n",
    "\n",
    "Inputs:\n",
    "- xn - matrix of samples of outcomes x. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate X.\n",
    "- yn - matrix of samples of outcomes y. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate Y.\n",
    "       Must have the same number of rows as X.\n",
    "- zn - matrix of samples of outcomes z. May be a 1D vector of samples, or\n",
    "       a 2D matrix, where each row is a vector sample for a multivariate Z\n",
    "       which will be conditioned on.\n",
    "       Must have the same number of rows as X.\n",
    "\n",
    "Outputs:\n",
    "- result - conditional mutual information of X with Y, given Z\n",
    " \n",
    "Copyright (C) 2020, Joseph T. Lizier. Distributed under GNU General Public License v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalmutualinformationempirical(xn,yn,zn):\n",
    "    \"\"\"    # Should we check any potential error conditions on the input?\n",
    "    if (isvector(xn)):\n",
    "        #Â Convert it to column vector if not already:\n",
    "        if (size(xn,1) == 1):\n",
    "            # xn has only one row:\n",
    "            xn = xn.T # Transpose it so it is only column\n",
    "    \n",
    "    if (isvector(yn)):\n",
    "        # Convert it to column vector if not already:\n",
    "        if (size(yn,1) == 1):\n",
    "            # yn has only one row:\n",
    "            yn = yn.T # Transpose it so it is only column\n",
    "\n",
    "    if (isvector(zn)):\n",
    "        # Convert it to column vector if not already:\n",
    "        if (size(zn,1) == 1):\n",
    "            # zn has only one row:\n",
    "            zn = zn.T # Transpose it so it is only column\n",
    "\n",
    "    # Check that their number of rows are the same:\n",
    "    assert(size(xn,1) == size(yn,1))\n",
    "    assert(size(xn,1) == size(zn,1))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate xn and yn to compute conditional entropy empircal\n",
    "    [c1,r1] = xn.shape\n",
    "    [c2,r2] = yn.shape\n",
    "    \n",
    "    if c1 == 1 and c2 == 1:\n",
    "        xy = np.concatenate((xn,yn), axis=0)   \n",
    "    else:\n",
    "        xy = np.concatenate((xn,yn),axis=1)\n",
    "\n",
    "\n",
    "    # We need to compute H(X|Z) + H(Y|Z) - H(X,Y|Z):\n",
    "    # 1. conditional joint entropy:\n",
    "    H_XY_given_Z = conditionalentropyempirical(xy, zn)\n",
    "    # 2. conditional entropy of Y:\n",
    "    H_Y_given_Z = conditionalentropyempirical(yn, zn)\n",
    "    # 3. conditional entropy of X:\n",
    "    H_X_given_Z = conditionalentropyempirical(xn, zn)\n",
    "\n",
    "    result = H_X_given_Z + H_Y_given_Z - H_XY_given_Z\n",
    "\n",
    "    # Alternatively, note that we could compute I(X;Y,Z) - I(X;Z)\n",
    "    # 1. joint MI:\n",
    "    # I_X_YZ = mutualinformationempirical(xn, [yn, zn])\n",
    "    # 2. MI just from Z:\n",
    "    # I_X_Z = mutualinformationempirical(xn, zn)\n",
    "    # Then:\n",
    "    # result = I_X_YZ - I_X_Z\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
