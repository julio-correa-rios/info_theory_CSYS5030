{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook week 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This week we will analyse the uncertainties and information contents of some sample Scissors-Paper-Rock gameplay.\n",
    "\n",
    "**Initial Questions:**\n",
    "\n",
    "Why are we interested in using measures of information theory to analyse this data set?\n",
    "What in particular might we wish to measure?\n",
    "Information theory is all about questions and answers. What questions might we ask of the data? What hypotheses might we have about the answers?\n",
    "\n",
    "### <p style=\"color:darkblue\">Stage 1 -- Familiarisation</p>\n",
    "\n",
    "I've done a lot of the data plumbing for you, so that we can concentrate on computing the information-theoretic quantities. Data plumbing is an important part of any analysis though, so do take a look in more detail at how the code was set up at some point. For now though, let's get things working and start to explore the data set.\n",
    "\n",
    "1. Please download a copy of the code templates and the data set above. Unzip them to any convenient location on your computer. You can also download the solution code, though I trust you not to go straight to it until you've had an attempt at the task first!\n",
    "2. Open the folder where the gameplay data set is stored. Open any file in a text editor, which includes the data for a game between two named players. The file contains each iteration of the game on one line, with {0,1,2} encoding the player's selections amongst {scissors,paper,rock}.\n",
    "3. Open the folder where your Scissors-Paper-Rock code templates are stored. Edit the file setup.m to point to the appropriate paths for the following. Note: you need to have a trailing '/' at the end of the paths!\n",
    "    - The Scissors-Paper-Rock data files;\n",
    "    - Your Matlab entropy scripts from the previous module (if you are confident that they are working, or else the completed code solutions).\n",
    "4. Run listPlayers to list out which player names you can analyse. You can alternatively run players = listPlayers(); to get a cell array of players that can be analysed. You can access each player name after that function call via players[1], players[2], e.g. up to len(players)\n",
    "5. Run loadGamesForPlayer(name), where name is the name string for any player (e.g. 'Joe'), to display the games (including moves and results) for that player. Note: You can call loadGamesForPlayer('*'), i.e. with name '*', to get the data for all players. The function can be called as games = loadGamesForPlayer(name); to return a cell array of the data for each game for that player, which will be used in our information theoretic analysis later. Each cell, e.g. games{1}, is a 2D array for the given game number, where:\n",
    "    - the first column is the moves of the named player ({0,1,2} for {scissors,paper,rock}),\n",
    "    - the second column is the moves of their opponent, and\n",
    "    - the third column is the result for this player ({1,0,-1} for {win,tie,loss}).\n",
    "\n",
    "### <p style=\"color:darkblue\">Stage 2 -- Entropy calculations</p>\n",
    "\n",
    "We will analyse the uncertainty in various player's moves using Shannon entropy, and consider whether this relates to their performance in the game. (Do you have a hypothesis on this?)\n",
    "\n",
    "1. Open the Matlab file computeEntropyForPlayer.m in your code templates location. This aims to compute the entropy of moves for a given named player, over all the iterations in all of their games. The code retrieves the data for each game of this player using loadGamesForPlayer(), then loops over each game. Fill out the missing parts of code:\n",
    "    - In the loop, pull out the moves for that player (and their results), and append them into the arrays used to store these values over all iterations. A helpful hint is that if you had a 2D matrix data, and you wanted to pull out column 1 of its contents, you would do this as: data(:,1).\n",
    "    - Compute the entropy over the players' moves, using our entropyempirical script.\n",
    "\n",
    "2. Call the script for a few different players, e.g. computeEntropyForPlayer('Joe'), and compare.\n",
    "\n",
    "3. Now call it using all players' data at once, in a single calculation: computeEntropyForPlayer('*'). What implicit assumption are we making when we analyse the data in this way?\n",
    "\n",
    "4. Open the Matlab file computeEntropyForAllPlayers.m. This aims to compute entropy of moves for each player in turn (considering each player separately), then plots these, and looks for relationships between entropy and win/loss rates. Fill out the missing parts of code:\n",
    "In the loop over player names, use our previous script computeEntropyForPlayer to compute the entropy for that player.\n",
    "Once we have the entropy for each player and their win / loss ratios, compute the correlation between entropy and win ratio, and entropy and loss ratio. HINT: Use Matlab's inbuilt correlation function corrcoef or corr.\n",
    "\n",
    "5. Call the script to see the entropies of each player, the plots and correlation analyses on how this related to performance. Whose moves was there most uncertainty about? Did this correlate to wins? What about losses? Does this match your hypothesis?\n",
    "\n",
    "6. Challenge: are these correlation values statistically significant? Look up theory on how to compute whether a correlation value is statistically significant. To add this to the code, you can check out the other return values from Matlab's inbuilt correlation function.<br>\n",
    "\n",
    "We will continue to investigate relationships between variables in this data set once we have learned about the mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Coding mutual information\n",
    "\n",
    "**Continuing with the Information Theory basics**\n",
    "\n",
    "In this exercise we continue to alter the Python code to measure the mutual information between variables x and y for a distribution p(x,y):\n",
    "\n",
    "$I\\left(X;Y\\right)=H\\left(X\\right)+H\\left(Y\\right)-H\\left(X,Y\\right)$\n",
    "\n",
    "Your task is to edit the Python function mutualinformation(p) in the next cell to return the mutual information for the given distribution $p(x,y)$ over joint outcomes ${x,y}$ of variables X,Y.\n",
    "\n",
    "As before, the input argument to the function is a matrix p, representing the probability mass for each joint outcome of ${x,y}$. That is, p is a matrix with the (i,j)th entry in the matrix giving the probability for the joint outcome of the ith value that x may take along with the jth value that y may take. The sum of the items in the matrix <b>p</b> must be 1. For example, for a binary x and y we could have <b>p = np.array([[0.2, 0.3],[ 0.1, 0.4]])</b> where $p(x=0,y=0) = 0.2$, $p(x=0,y=1) = 0.3$, $p(x=1,y=0) = 0.1$, and $p(x=1,y=1) = 0.4$. If the variable <b>$x$</b> can take more than two values for example, then we will have more than two rows in <b>p</b>.\n",
    "<br>\n",
    "\n",
    "6.1. To fill in the template, you will need to call your existing functions entropy(p) for H(X,Y) and jointentropy(p) for H(X) and H(Y) to provide the calculations needed. Note that to compute H(Y) you will need to extract p(y) from the p(x,y) matrix by summing over all x rows (as per the activity for conditional entropy in the previous module), whilst for H(X) you will need to extract p(x) from the p(x,y) matrix by summing over all y columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualinformation(p):\n",
    "    \n",
    "    # We need to compute H(X) + H(Y) - H(X,Y):\n",
    "    # 1. joint entropy:\n",
    "    H_XY # = ???\n",
    "\n",
    "    # 2. marginal entropy of X:\n",
    "    # But how to get p_x???\n",
    "    p_x # =  ??? \n",
    "    H_X # =  ???\n",
    "\n",
    "    # 2. marginal entropy of Y:\n",
    "    # But how to get p_y???\n",
    "    p_y # = ???\n",
    "    H_Y # = ??? \n",
    "    \n",
    "    mutualInfo = H_X + H_Y - H_XY\n",
    "    \n",
    "    return np.round(mutualInfo,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2. Test that your code works, e.g. by running:<br>\n",
    "    \n",
    "    a. mutualinformation(np.array([[0.2, 0.3],[ 0.1, 0.4]])) and validating that you get the result 0.0349 bits. Recall that the conditional entropy computed for this p(x,y) probability table in the previous module was 0.965 bits - is the result for MI sensible with respect to that result?\n",
    "    b. mutualinformation(np.array([[0.5, 0],[ 0, 0.5]])) and validating that you get the result 1 bit. <br>\n",
    "    c. mutualinformation(np.array([[0.25, 0.25],[ 0.25, 0.25]])) and validating that you get the result 0 bits. Can you explain this and the previous result? \n",
    "<br>\n",
    "\n",
    "\n",
    "6.3. Coming back to the Guess Who? example using the [Kooky character sheet](https://www.hasbro.com/upload/guesswho/GWc_Kooky-en_GB.pdf):\n",
    "<br>\n",
    "    \n",
    "    a. Compute the mutual information between whether the character has horns and whether they have eyebrows, i.e. I(horns ; eyebrows)? As per the exercise in the previous module, construct first the table p(horns, eyebrows) for all 4 combinations of these two binary variables, then pass this to your function.\n",
    "    b. From your constructed table p(horns, eyebrows), first construct the distribution p(eyebrows) and evaluate p(eyebrows = true). Next, construct the conditional probability distribution p(horns | eyebrows), and then evaluate the conditional probabilities given that the character has horns, i.e. p(eyebrows | horns = true). Finally, evaluate p(eyebrows = true | horns = true), and then use p(eyebrows = true | horns = true) and p(eyebrows = true) to compute i(eyebrows = true ; horns = true). Use your result to explain how helpful or unhelpful knowing that the character has horns, i.e. horns = true, was in determining whether the character had eyebrows, i.e. eyebrows = true.\n",
    "    c. Is I(eyebrows ; horns) the same? Recall that H(horns | eyebrows) != H(eyebrows | horns), so explain why the result for MI is the same or different? \n",
    "    d. Challenge: Can you find a pair of traits (or pairs of sets of traits) that appear to have high mutual information? What does it mean for these traits to have high mutual information in the individuals in this sheet?\n",
    "    \n",
    "\n",
    "6.4. Finally, let's code mutual information I(X;Y) for empirical samples xn and yn in the file mutualinformationempirical.m. Hint: You can call your existing code jointentropyempirical and entropyempirical to compute H(X,Y), H(X) and H(Y) respectively, by passing in [xn,yn], xn and yn as arguments to these functions respectively. Test that your code works by running, e.g.:\n",
    "\n",
    "    \n",
    "    a. <b>mutualinformationempirical(np.array([[0,0,1,1]],[0,1,0,1]]))</b> and validating that you get the result 0 bits.\n",
    "    b. <b>mutualinformationempirical(np.array([[0,0,1,1],[0,0,1,1]]))</b> and validating that you get the result 1 bit.\n",
    "    c. Can you explain the expected results for these boundary cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
