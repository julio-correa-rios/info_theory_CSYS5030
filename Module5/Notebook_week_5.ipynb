{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Install JIDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install JIDT on your own machine, or the PC in the classroom:\n",
    "\n",
    "1. Download a zip file of the latest release via option 2 at the [Installation](https://github.com/jlizier/jidt/wiki/Installation) page. (Expert users could take a clone or fork of the [git repository](https://github.com/jlizier/jidt) instead.)\n",
    "2. Unzip the zip file to a location of your choosing. If you are doing this on the PC in the classroom, you may wish to download to a USB stick so that you can reuse the installation at the next class, or on another PC, etc. Also, if I update the code, e.g. to provide new examples or functions for class, then you can unzip any new distribution straight over the top of the old one.\n",
    "3. If installing on your own machine, check dependencies 1 and 4 listed at the [Installation](https://github.com/jlizier/jidt/wiki/Installation) page (2 and 3 only apply to expert users). Note that dependency 1 explicitly means a Java development kit, not the runtime environment (i.e. if you can run java from the command line that's not enough, you need to also be able to run javac). You will not have any further dependencies if you only intend to use the toolkit from Matlab (instructions on installing Matlab on your personal machine are available via Module 0). Otherwise, if you wish to run it in Python (or R etc, but far less support is provided for these, and none in class!) you may have further dependencies. Please follow the links from Dependency 4 for Python or Octave, or for other environments if you wish to use them from the [main](https://github.com/jlizier/jidt) JIDT page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bias and variance and extending JIDT code\n",
    "\n",
    "This activity illustrates bias and variance of MI estimates, and gets you started extending the code generated by the JIDT AutoAnalyser.\n",
    "\n",
    "1. Start by generating the code again for the demonstration of the Discrete MI on slide 20 of Session 3 Introduction to JIDT.\n",
    "<br>\n",
    "\n",
    "2. Open the generated Python code in the file demos/AutoAnalyser/GeneratedCalculator.py, or copy and past the code from the Python tab into either a new .py file or a Jupyter notebook (can be placed anywhere).\n",
    "<br>\n",
    "\n",
    "3. Edit step 0 of the code which loads the data in to the source and destination variables:<br>\n",
    "    a. First remove the line where the file is loaded.\n",
    "    <br>\n",
    "    b. Next, change the assignment of the source variable to be an array of 10 random bits: source = np.random.randint(0,2,size=10).<br>\n",
    "\n",
    "    c. Finally, change the assignment of the destination variable to be a copy of the source: destination = source;\n",
    "    Congratulations, you have made your first extension of the automatically generated JIDT code! Now run the code in Python. Start Python (if you're not already editing the .ipynb file after launching your Jupyter notebook), and change the working directory to demos/AutoAnalyer/. Run GeneratedCalculator (or the alternative name of your script) as you run every single file.\n",
    "<br>\n",
    "\n",
    "4. Note the result. Was it the full 1 bit of shared information the we would expect for copied random bits?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.\n",
    "\n",
    "5. Run the code several more times and note the results. Are they always the same or do they vary? Why is this?\n",
    "<br>\n",
    "\n",
    "6. Capture the results of running the code several times (say 10 times) into an array and measure the mean and variance of the results. (Hint: in Python, there are different ways to do it, one of the simplest is to append the results in a list, and it should look like this: <b>results.append(yourCalculation)</b>). You would be best to use a <b>for</b> loop to run the code 10 times). Compute the bias as the difference between the mean empirical result and the expected result. It is quite large here because we have computed the empirical results from so few samples (10). In the lecture we noted that MI is typically biased upwards, which referred to situations where variables don't actually share any information; where variables do indeed share information, the MI can be biased downwards as is the case here. Also try to increase the number of samples (e.g. upwards from 10 random bits to 100) and see how the bias and variance change.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
