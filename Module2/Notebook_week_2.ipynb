{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook week two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Coding Shannon entropy for empirical data<br>\n",
    "\n",
    "In this exercise we continue to alter the Python code in the next cell to measure the Shannon entropy. This time, let's code it not from a given distribution p(x), but from empirical data of samples x of the variable X.\n",
    "\n",
    "Your task is to edit the function entropyempirical(xn) in the next cell to return the Shannon entropy for the given samples  ùë•ùëõ  of X (n is the sample index). Note that the input  ùë•ùëõ  is a vector, with each entry representing one sample.\n",
    "\n",
    "Examine the code template in the next cell. The first task the code performs is to work out the alphabet A_X (contained in the variable symbols) that the samples are drawn from. Then the code template counts the number of occurrences of each symbol of the alphabet in the samples, normalises those counts into probabilities, and then computes the entropy from that. Fill out the code to perform these tasks where indicated with ???.\n",
    "\n",
    "\n",
    "Test your code on some vectors of empirical data, e.g. entropyempirical([0,0,1,1]) should return 1 bit. Design other test data sets where you know what the result should be, and test them.\n",
    "\n",
    "\n",
    "What do you expect the average entropy of coin tosses to be? Toss a coin yourself 10 times, recording the results for each toss, and create a vector of boolean values to represent these samples. Call entropyempirical with this vector of samples -- did it return the result you expected? Try your experiment again and see if the result changed. Explain your results here.\n",
    "\n",
    "\n",
    "Create boolean samples from random data, e.g. with randi(2, 1, 10), and call entropyempirical with this vector of samples. Again -- does it return the result you expected? Try longer data sets, and also samples drawn from larger alphabets, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyempirical(xn):\n",
    "    \n",
    "    # We need to work out the alphabet here.\n",
    "    # The following returns a vector of the alphabet:\n",
    "    \n",
    "    if type(xn) == list:\n",
    "        xn = np.array(xn)\n",
    "    \n",
    "    if xn.ndim == 1:\n",
    "        xn = np.reshape(xn,(1,len(xn))) #reshaping our 1-dim vector to numpy format of a row vector\n",
    "        xn = xn.T # Transposing the row vector to a column vector\n",
    "    \n",
    "    [c1,r1] = xn.shape \n",
    "    denominator = c1\n",
    "    \n",
    "    if c1 == 1:\n",
    "        xn = xn.T\n",
    "        [r1,c1] = xn.shape\n",
    "        denominator = r1\n",
    "        \n",
    "    unique_rows = np.unique(xn, axis=0)\n",
    "    probabilities = []\n",
    "    for unique_row in unique_rows:\n",
    "        count = 0\n",
    "        for row in xn:\n",
    "            if (row==unique_row).all():\n",
    "                count += 1       \n",
    "        probabilities.append(count/denominator)\n",
    "\n",
    "    empEntropy = ??? # 1. Compute Shannon Entropy for Empirical Data\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Coding Joint entropy\n",
    "\n",
    "In this exercise we continue to alter the code templatesPreview the document to measure the joint entropy for a distribution p(x,y):\n",
    "\n",
    "$H\\left(X,Y\\right)=-\\sum_{x,y}p\\left(x,y\\right)\\log p\\left(x,y\\right)$\n",
    "\n",
    "Your task is to edit the function <b>jointentropy(p)</b> in the next cell to return the Shannon entropy for the given distribution p(x,y) over joint outcomes $\\{x,y\\}$ of variables X,Y.\n",
    "\n",
    "Note the input argument to the function is a matrix <b>p</b>, representing the probability mass for each joint outcome of ${x,y}$. That is, <b>p</b> is a matrix with the (i,j)th entry in the matrix giving the probability for the joint outcome of the ith value that x may take along with the jth value that y may take. The sum of the items in the matrix p must be 1.\n",
    "\n",
    "For example, for a binary x and y we could have <b> $p = [0.2, 0.3; 0.1, 0.4]$ </b> where p(x=0,y=0) = 0.2, p(x=0,y=1) = 0.3, p(x=1,y=0) = 0.1, and p(x=1,y=1) = 0.4. If the variable x can take more than two values for example, then we will have more than two rows in p.\n",
    "\n",
    "1. To get started, think about whether you can make simple changes to your code from <b>entropy(p)</b> to extend it to work here.\n",
    "<br>\n",
    "\n",
    "2. Test that your code works, e.g. by running <b>jointentropy(np.array([0.2, 0.3],[ 0.1, 0.4]))</b> and validating that you get the result 1.85 bits. Come up with some other test cases to check, e.g. could you check similar boundary cases to what we used to test <b>entropy(p)</b> in the previous module?\n",
    "<br>\n",
    "\n",
    "3. Challenge: try dropping the assumption that the input argument <b>p</b> is of 2 dimensions, but allow it to be a matrix of arbitrary dimensions. Can you do this with minimal changes to the code? Hint: calling p(:) where <b>p</b> is a matrix of any number of dimensions will return a one dimensional array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jointentropy(p):\n",
    "    \n",
    "    # Do I have to test whether this sums up 1?\n",
    "    \n",
    "    joint_entropy = ??? # 1. Compute Joint Entropy\n",
    "    \n",
    "    return ??? # 2. Determine the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Extension) Coding joint entropy for empirical data\n",
    "\n",
    "We continue with the Python code templatesPreview the document to measure the joint entropy from empirical data of samples x of the variable X.\n",
    "\n",
    "This is already implemented in this Python notebook <b>jointentropyempirical(xn)</b>, to return the joint entropy for the given samples x_n of X (n is the sample index). Note that the input xn is a matrix, where rows represent samples and columns represent variables; i.e. <b>xn=[0,1; 1,1; 1,0]</b> represents 3 samples of 2 variables.\n",
    "\n",
    "1. Examine the code in the next cell. Most of the code actually pre-processes the input arguments, before it maps a distinct symbol for each unique row in xn and then asks entropyempirical() to calculate the entropy.\n",
    "2. Test the code on some vectors of empirical data, e.g. <b>jointentropyempirical([0,1;0,0;1,0;1,1])</b> should return 2 bits since we provided 4 distinct equiprobable samples. Design other test data sets where you know what the result should be, and test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jointentropyempirical(xn, yn):\n",
    "    \n",
    "    xn = np.array(xn)\n",
    "    yn = np.array(yn)\n",
    "    \n",
    "    if xn.ndim == 1:\n",
    "        xn = np.reshape(xn,(1,len(xn)))\n",
    "    \n",
    "    if yn.ndim == 1:\n",
    "        yn = np.reshape(yn,(1,len(yn)))\n",
    "    \n",
    "    [c1,r1] = xn.shape\n",
    "    [c2,r2] = yn.shape\n",
    "    \n",
    "    if c1 == 1 and c2 == 1:\n",
    "        XY = np.concatenate((xn,yn), axis=0)\n",
    "        jointEntrEmp = ??? # 1. Compute Joint Entropy\n",
    "        return ??? # 2. Result\n",
    "    \n",
    "    elif c1 != c2:\n",
    "        XY = np.row_stack((xn,yn))\n",
    "        jointEntrEmp = ??? # 3. Compute Joint Entropy\n",
    "        return ??? # 4. Result\n",
    "    \n",
    "    else:\n",
    "        XY = np.concatenate((xn,yn),axis=1)\n",
    "        jointEntrEmp = ??? # 5. Compute Joint Entropy\n",
    "        return ??? # 6. Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Coding Conditional entropy\n",
    "In this exercise we continue to alter the Pyhton code templatesPreview the document to measure the conditional entropy for a distribution p(x,y):\n",
    "\n",
    " $\\begin{aligned}\n",
    "H\\left(X\\mid Y\\right)=&-\\sum_{x,y}p\\left(x,y\\right)\\log p\\left(x\\mid y\\right)\\\\\n",
    "=&-\\sum_{x,y}p\\left(x,y\\right)\\left(\\log p\\left(x,y\\right)-\\log p\\left(y\\right)\\right)\\\\\n",
    "=&H\\left(X,Y\\right)-H\\left(Y\\right)\n",
    "\\end{aligned}$\n",
    "<br>\n",
    "\n",
    "Your task is to edit the Python function conditionalentropy(p) in the next cell to return the conditional entropy for the given distribution p(x,y) over joint outcomes {x,y} of variables X,Y.\n",
    "\n",
    "As above for the joint entropy, the input argument to the function is a matrix p, representing the probability mass for each joint outcome of {x,y}.\n",
    "\n",
    "5.1. To fill in the template, you will need to call your existing functions entropy(p) for H(X,Y) and jointentropy(p) for H(Y) to provide the calculations needed. Note that to compute H(Y) you will need to extract p(y) from the p(x,y) matrix by summing over all x rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalentropy(p):\n",
    "    \n",
    "    p = np.array(p)\n",
    "\n",
    "    H_XY = ??? # 1. Compute Joint entropy between X and Y\n",
    "    \n",
    "    p1, p2 = p.sum(axis=0) \n",
    "    p_primma = np.array([p1,p2])\n",
    "    \n",
    "    H_Y = # 1. Compute Entropy of Y\n",
    "\n",
    "    return ??? # 3. Define result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
